{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje *out-of-core*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemas de escalabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las clases `sklearn.feature_extraction.text.CountVectorizer` y `sklearn.feature_extraction.text.TfidfVectorizer` tienen una serie de problemas de escalabilidad que provienen de la forma en que se utiliza, a nivel interno, el atributo `vocabulary_` (que es un diccionario Python) para convertir los nombres de las características (cadenas) a índices enteros de características.\n",
    "\n",
    "Los principales problemas de escalabilidad son:\n",
    "\n",
    "- **Uso de memoria del vectorizador de texto**: todas las representaciones textuales de características se cargan en memoria.\n",
    "- **Problemas de paralelización para extracción de características**: el atributo `vocabulary_` es compartido, lo que conlleva que sea difícil la sincronización y por tanto que se produzca una sobrecarga.\n",
    "- **Imposibilidad de realizar aprendizaje *online*, *out-of-core* o *streaming***: el atributo `vocabulary_` tiene que obtenerse a partir de los datos y su tamaño no se puede conocer hasta que no realizamos una pasada completa por toda la base de datos de entrenamiento.\n",
    "\n",
    "Para entender mejor estos problemas, analicemos como trabaja el atributo `vocabulary_`. En la fase de `fit` se identifican los tokens del corpus de forma unívoca, mediante un índice entero, y esta correspondencia se guarda en el vocabulario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "vectorizer.fit([\n",
    "    \"The cat sat on the mat.\",\n",
    "])\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El vocabulario se utiliza en la fase `transform` para construir la matriz de ocurrencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.transform([\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"This cat is a nice cat.\",\n",
    "]).toarray()\n",
    "\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a realizar un nuevo `fit` con un corpus algo más grande:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "vectorizer.fit([\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "])\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El atributo `vocabulary_` crece (en escala logarítmica) con respecto al tamaño del conjunto de entrenamiento. Observa que no podemos construir los vocabularios en paralelo para cada documento de texto ya que hay algunas palabras que son comunes y necesitaríamos alguna estructura compartida o barrera de sincronización (aumentando la complejidad de implementar el entrenamiento, sobre todo si queremos distribuirlo en un cluster).\n",
    "\n",
    "Con este nuevo vocabulario, la dimensionalidad del espacio de salida es mayor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.transform([\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"This cat is a nice cat.\",\n",
    "]).toarray()\n",
    "\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El dataset de películas IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mostrar los problemas de escalabilidad con los vocabularios basados en vectorizadores, vamos a cargar un dataset realista que proviene de una tarea típica de clasificación de textos: análisis de sentimientos en texto. El objetivo es discernir entre revisiones positivas y negativas a partir de la base de datos de [Internet Movie Database](http://www.imdb.com) (IMDb).\n",
    "\n",
    "En las siguientes secciones, vamos a usar el siguiente dataset [subset](http://ai.stanford.edu/~amaas/data/sentiment/) de revisiones de películas de IMDb, que has sido recolectado por Maas et al. \n",
    "\n",
    "- A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning Word Vectors for Sentiment Analysis. In the proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. \n",
    "\n",
    "Este dataset contiene 50,000 revisiones de películas, divididas en 25,000 ejemplos de entrenamiento y 25,000 ejemplos de test. Las revisiones se etiquetan como negativas (`neg`) o positivas (`pos`). De hecho, las negativas recibieron $\\le 4$ estrellas en IMDb; las positivas recibieron $\\ge 7$ estrellas. Las revisiones neutrales no se incluyeron en el dataset.\n",
    "\n",
    "Asumiendo que ya habéis ejecutado el *script* `fetch_data.py`, deberías tener disponibles los siguientes ficheros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_path = os.path.join('datasets', 'IMDb', 'aclImdb', 'train')\n",
    "test_path = os.path.join('datasets', 'IMDb', 'aclImdb', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a cargarlos en nuestra sesión activa usando la función `load_files` de scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "train = load_files(container_path=(train_path),\n",
    "                   categories=['pos', 'neg'])\n",
    "\n",
    "test = load_files(container_path=(test_path),\n",
    "                  categories=['pos', 'neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>NOTA</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Ya que el dataset de películas contiene 50,000 ficheros individuales de texto, ejecutar el código anterior puede llevar bastante tiempo.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `load_files`  ha cargado los datasets en objetos `sklearn.datasets.base.Bunch`, que son diccionarios de Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En particular, solo estamos interesados en los arrays `data` y `target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for label, data in zip(('ENTRENAMIENTO', 'TEST'), (train, test)):\n",
    "    print('\\n\\n%s' % label)\n",
    "    print('Número de documentos:', len(data['data']))\n",
    "    print('\\n1er documento:\\n', data['data'][0])\n",
    "    print('\\n1era etiqueta:', data['target'][0])\n",
    "    print('\\nNombre de las clases:', data['target_names'])\n",
    "    print('Conteo de las clases:', \n",
    "          np.unique(data['target']), ' -> ',\n",
    "          np.bincount(data['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes comprobar, el array `'target'` consiste en valores `0` y `1`, donde el `0` es una revisión negativa y el `1` representa una positiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El truco del *hashing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerda la representación *bag-of-words* que se obtenía usando un vectorizador basado en vocabulario:\n",
    "\n",
    "<img src=\"figures/bag_of_words.svg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para solventar las limitaciones de los vectorizadores basados en vocabularios, se puede utilizar el truco del *hashing*. En lugar de construir y almacenar una conversión explícita de los nombres de las características a los índices de las mismas dentro de un diccionario Python, podemos aplicar una función de *hash* y el operador módulo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/hashing_vectorizer.svg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos acceder a más información y a las referencias a los artículos originales en el siguiente [sitio web](http://www.hunch.net/~jl/projects/hash_reps/index.html), y una descripción más sencilla en este otro [sitio](http://blog.someben.com/2013/01/hashing-lang/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.murmurhash import murmurhash3_bytes_u32\n",
    "\n",
    "# Codificado para compatibilidad con Python 3\n",
    "for word in \"the cat sat on the mat\".encode(\"utf-8\").split():\n",
    "    print(\"{0} => {1}\".format(\n",
    "        word, murmurhash3_bytes_u32(word, 0) % 2 ** 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La conversión no tiene estado y la dimensionalidad del espacio de salida se fija a priori (aquí usamos módulo `2 ** 20`, que significa aproximadamente que tenemos un millón de dimensiones, $2^{20}$). Esto hace posible evitar las limitaciones del vectorizador de vocabulario, tanto a nivel de paralelización como de poder aplicar aprendizaje *online*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase `HashingVectorizer` es una alternativa a `CountVectorizer` (o a `TfidfVectorizer` si consideramos `use_idf=False`) que aplica internamente la función de *hash* llamada ``murmurhash``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "h_vectorizer = HashingVectorizer(encoding='latin-1')\n",
    "h_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparte la misma estructura de preprocesamiento, generación de tokens y análisis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = h_vectorizer.build_analyzer()\n",
    "analyzer('Esta es una frase de prueba.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos vectorizar nuestros datasets en matriz dispersa de scipy de la misma forma que hubiéramos hecho con `CountVectorizer` o `TfidfVectorizer`, excepto que podemos llamar directamente al método `transform`. No hay necesidad de llamar a `fit` porque el `HashingVectorizer` no se entrena, las transformaciones están prefijadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_train, y_train = train['data'], train['target']\n",
    "docs_valid, y_valid = test['data'][:12500], test['target'][:12500]\n",
    "docs_test, y_test = test['data'][12500:], test['target'][12500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dimensión de salida se fija de antemano a `n_features=2 ** 20` (valor por defecto) para minimizar la probabilidad de colisión en la mayoría de problemas de clasificación (1M de pesos en el atributo `coef_`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_vectorizer.transform(docs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a comparar la eficiencia computacional de `HashingVectorizer` con respecto a `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_vec = HashingVectorizer(encoding='latin-1')\n",
    "%timeit -n 1 -r 3 h_vec.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec =  CountVectorizer(encoding='latin-1')\n",
    "%timeit -n 1 -r 3 count_vec.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes observar, ``HashingVectorizer`` es mucho más rápido que ``Countvectorizer``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, vamos a entrenar un clasificador ``LogisticRegression`` en los datos de entrenamiento de IMDb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "h_pipeline = Pipeline([\n",
    "    ('vec', HashingVectorizer(encoding='latin-1')),\n",
    "    ('clf', LogisticRegression(random_state=1)),\n",
    "])\n",
    "\n",
    "h_pipeline.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy de entrenamiento', h_pipeline.score(docs_train, y_train))\n",
    "print('Accuracy de validación', h_pipeline.score(docs_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del count_vec\n",
    "del h_pipeline\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje *Out-of-Core*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aprendizaje *Out-of-Core* consiste en entrenar un modelo de aprendizaje automático usando un dataset que no cabe en memoria RAM. Requiere las siguientes condiciones:\n",
    "    \n",
    "- Una capa de **extracción de características** con una **dimensionalidad de salida fija**.\n",
    "- Saber la lista de clases de antemano (en este caso, sabemos que hay *tweets* positivos y negativos).\n",
    "- Un algoritmo de aprendizaje automático que soporte **aprendizaje incremental** (método `partial_fit` en scikit-learn).\n",
    "\n",
    "En la siguientes secciones, vamos a configurar una función simple de entrenamiento iterativo de un `SGDClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero primero cargamos los nombres de los ficheros en una lista de Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join('datasets', 'IMDb', 'aclImdb', 'train')\n",
    "train_pos = os.path.join(train_path, 'pos')\n",
    "train_neg = os.path.join(train_path, 'neg')\n",
    "\n",
    "fnames = [os.path.join(train_pos, f) for f in os.listdir(train_pos)] +\\\n",
    "         [os.path.join(train_neg, f) for f in os.listdir(train_neg)]\n",
    "\n",
    "fnames[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a crear el array de etiquetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.zeros((len(fnames), ), dtype=int)\n",
    "y_train[:12500] = 1\n",
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a implementar la función `batch_train function`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "def batch_train(clf, fnames, labels, iterations=25, batchsize=1000, random_seed=1):\n",
    "    vec = HashingVectorizer(encoding='latin-1')\n",
    "    idx = np.arange(labels.shape[0])\n",
    "    c_clf = clone(clf)\n",
    "    rng = np.random.RandomState(seed=random_seed)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        rnd_idx = rng.choice(idx, size=batchsize)\n",
    "        documents = []\n",
    "        for i in rnd_idx:\n",
    "            with open(fnames[i], 'r') as f:\n",
    "                documents.append(f.read())\n",
    "        X_batch = vec.transform(documents)\n",
    "        batch_labels = labels[rnd_idx]\n",
    "        c_clf.partial_fit(X=X_batch, \n",
    "                          y=batch_labels, \n",
    "                          classes=[0, 1])\n",
    "      \n",
    "    return c_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a utilizar la clase un `SGDClassifier` con un coste logístico en lugar de `LogisticRegression`. SGD proviene de *stochastic gradient descent*, un algoritmo de optimización que optimiza los pesos de forma iterativa ejemplo a ejemplo, lo que nos permite pasarle los datos en grupos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como empleamos el `SGDClassifier` con la configuración por defecto, entrenará el clasificador en 25\\*1000=25000 documentos (lo que puede llevar algo de tiempo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = SGDClassifier(loss='log', random_state=1)\n",
    "\n",
    "sgd = batch_train(clf=sgd,\n",
    "                  fnames=fnames,\n",
    "                  labels=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al terminar, evaluemos el rendimiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = HashingVectorizer(encoding='latin-1')\n",
    "sgd.score(vec.transform(docs_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitaciones de los vectorizadores basados en *hash*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar este tipo de vectorizadores nos permiten una mejor escalabilidad y trabajar *on-line*, pero también tienen algunos inconvenientes:\n",
    "    \n",
    "- Las colisiones podrían introducir ruido en los datos y degradar la calidad de las predicciones.\n",
    "- La clase `HashingVectorizer` no nos permite emplear \"*Inverse Document Frequency*\" (no existe la opción `use_idf=True`).\n",
    "- No hay una forma simple de realizar una conversión inversa y encontrar los nombres de las características a partir del índice.\n",
    "\n",
    "Las colisiones pueden minimizarse incrementando el parámetro `n_features`.\n",
    "\n",
    "El peso IDF podría reintroducirse si añadimos un objeto de la clase `TfidfTransformer` a la salida del vectorizador. Sin embargo, obtener el estadístico `idf_` utilizado para sopesar las características implicaría al menos una pasada adicional por los datos de entrenamiento antes de empezar a entrenar el clasificador: ya no podríamos afrontar un escenario *on-line*.\n",
    "\n",
    "La falta de una conversión inversa (método `get_feature_names()` de `TfidfVectorizer`) es mucho más difícil de evitar. Tendríamos que extender la clase `HashingVectorizer` para añadir un modo \"traza\" que nos permitiera guardar la conversión de las características más importantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EJERCICIO</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      En la implementación propuesta de la función ``batch_train``, se tomaron aleatoriamente *k* ejemplos de entrenamiento como *batch* en cada iteración, lo que puede considerarse un muestreo aleatorio con reemplazamiento. Modifica la función `batch_train` de forma que itere sobre todos los documentos ***sin*** reemplazamiento, es decir, que se usa cada documento ***una sola vez*** por iteración?\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
